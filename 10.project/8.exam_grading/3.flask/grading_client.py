#!/usr/bin/env python3
"""
MCP ì±„ì  í´ë¼ì´ì–¸íŠ¸ - ì±„ì  ë¡œì§ì„ ë‹´ë‹¹
MCP ì„œë²„ì™€ í†µì‹ í•˜ì—¬ íŒŒì¼ì„ ì½ê³  ê²°ê³¼ë¥¼ ì €ì¥
"""

import asyncio
import json
import os
import re
import subprocess
from typing import List, Dict, Any
import openai
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class ExamGradingClient:
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.openai_client = openai.AsyncOpenAI(api_key=api_key)
        self.exam_config = None
        self.results = []
    
    async def call_mcp_server(self, tool_name: str, arguments: Dict[str, Any] = None) -> Dict[str, Any]:
        """MCP ì„œë²„ì™€ ì§ì ‘ í†µì‹  (ëª…ë ¹í–‰ ë°©ì‹)"""
        try:
            if arguments is None:
                arguments = {}
                
            # ëª…ë ¹í–‰ ì¸ìˆ˜ êµ¬ì„±
            cmd = ["python", "file_server.py", tool_name]
            for key, value in arguments.items():
                cmd.extend([f"--{key}", str(value)])
            
            print(f"ğŸ”§ MCP í˜¸ì¶œ: {' '.join(cmd)}")
            
            # ì„œë²„ ì‹¤í–‰
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                encoding='utf-8'
            )
            
            if process.stderr:
                print(f"âš ï¸  ì„œë²„ ê²½ê³ : {process.stderr}")
            
            if process.stdout:
                try:
                    result = json.loads(process.stdout)
                    return result
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    print(f"ì›ì‹œ ì¶œë ¥: {process.stdout[:200]}...")
                    return {"error": f"JSON íŒŒì‹± ì‹¤íŒ¨: {process.stdout[:200]}"}
            
            return {"error": "ì„œë²„ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤"}
            
        except subprocess.TimeoutExpired:
            return {"error": "ì„œë²„ ì‘ë‹µ íƒ€ì„ì•„ì›ƒ"}
        except Exception as e:
            return {"error": f"MCP í†µì‹  ì˜¤ë¥˜: {e}"}
    
    async def load_exam_config(self) -> bool:
        """ì‹œí—˜ ì„¤ì • ë¡œë“œ"""
        print("ğŸ“š ì‹œí—˜ ì„¤ì • ë¡œë“œ ì¤‘...")
        result = await self.call_mcp_server("read_exam_config")
        
        if "error" in result:
            print(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {result['error']}")
            return False
        
        if result.get("success"):
            self.exam_config = result["config"]
            print(f"âœ… ì‹œí—˜ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
            print(f"   ğŸ“‹ ì‹œí—˜ëª…: {self.exam_config.get('exam_title', 'Unknown')}")
            print(f"   ğŸ“Š ì´ì : {self.exam_config.get('total_points', 100)}ì ")
            print(f"   ğŸ“ ë¬¸ì œ ìˆ˜: {len(self.exam_config.get('questions', []))}ê°œ")
            return True
        else:
            print(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {result}")
            return False
    
    async def get_answer_files(self) -> List[Dict[str, Any]]:
        """ë‹µì•ˆ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        print("ğŸ“ ë‹µì•ˆ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        result = await self.call_mcp_server("list_answer_files", {"pattern": "*.txt"})
        
        if "error" in result:
            print(f"âŒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {result['error']}")
            return []
        
        if result.get("success"):
            files = result["files"]
            print(f"âœ… {result['total_count']}ê°œ ë‹µì•ˆ íŒŒì¼ ë°œê²¬")
            for file_info in files:
                print(f"   ğŸ“„ {file_info['filename']} ({file_info['size']} bytes)")
            return files
        else:
            print(f"âŒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {result}")
            return []
    
    async def read_answer_file(self, filename: str) -> Dict[str, Any]:
        """íŠ¹ì • ë‹µì•ˆ íŒŒì¼ ì½ê¸°"""
        result = await self.call_mcp_server("read_answer_file", {"filename": filename})
        
        if "error" in result:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {filename}: {result['error']}")
            return {}
        
        if result.get("success"):
            return result
        else:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {filename}: {result}")
            return {}
    
    def build_grading_prompt(self, answer_content: str) -> str:
        """ì±„ì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if not self.exam_config:
            return "ì„¤ì • íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ë¬¸ì œ ì •ë³´ ì¶”ì¶œ
        questions = self.exam_config.get('questions', [])
        grading_criteria = self.exam_config.get('grading_criteria', {})
        
        # ë¬¸ì œ í…ìŠ¤íŠ¸ êµ¬ì„±
        questions_text = ""
        for i, question in enumerate(questions, 1):
            questions_text += f"\në¬¸ì œ {i}: {question.get('question', '')}\n"
            if question.get('points'):
                questions_text += f"ë°°ì : {question['points']}ì \n"
            if question.get('sample_answer'):
                questions_text += f"ëª¨ë²”ë‹µì•ˆ: {question['sample_answer']}\n"
        
        # ì±„ì ê¸°ì¤€ í…ìŠ¤íŠ¸ êµ¬ì„±
        criteria_text = ""
        for category, details in grading_criteria.items():
            criteria_text += f"\n{category}: {details.get('points', 0)}ì "
            if details.get('description'):
                criteria_text += f" - {details['description']}"
            if details.get('subcriteria'):
                for sub in details['subcriteria']:
                    criteria_text += f"\n  â€¢ {sub}"
        
        prompt = f"""
ë‹¤ìŒ ì‹œí—˜ì˜ í•™ìƒ ë‹µì•ˆì„ ì±„ì í•´ì£¼ì„¸ìš”.

=== ì‹œí—˜ ì •ë³´ ===
ì‹œí—˜ëª…: {self.exam_config.get('exam_title', 'ì‹œí—˜')}
ì´ì : {self.exam_config.get('total_points', 100)}ì 

=== ì¶œì œ ë¬¸ì œ ==={questions_text}

=== ì±„ì  ê¸°ì¤€ ==={criteria_text}

=== í•™ìƒ ë‹µì•ˆ ===
{answer_content}

=== ì±„ì  ìš”ì²­ ===
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ ìˆœìˆ˜í•œ JSONë§Œ ë°˜í™˜:

{{
    "total_score": ì´ì (0-{self.exam_config.get('total_points', 100)}),
    "question_scores": [
        {{
            "question_number": 1,
            "score": ì ìˆ˜,
            "max_score": ë§Œì ,
            "feedback": "ë¬¸ì œë³„ í”¼ë“œë°±"
        }}
    ],
    "criteria_scores": {{
        "ì •í™•ì„±": ì ìˆ˜,
        "ë…¼ë¦¬ì„±": ì ìˆ˜,
        "ëª…í™•ì„±": ì ìˆ˜,
        "ì°½ì˜ì„±": ì ìˆ˜,
        "í‘œí˜„ë ¥": ì ìˆ˜
    }},
    "overall_feedback": "ì „ì²´ì ì¸ í”¼ë“œë°±",
    "strengths": ["ì˜í•œ ì ë“¤"],
    "improvements": ["ê°œì„ í•  ì ë“¤"],
    "grade": "ë“±ê¸‰(A+, A, B+, B, C+, C, D+, D, F)"
}}

ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""
        return prompt
    
    def clean_json_response(self, response_content: str) -> str:
        """OpenAI ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
        cleaned_content = response_content.strip()
        
        # ì½”ë“œ ë¸”ë¡ ì œê±°
        if cleaned_content.startswith('```json'):
            cleaned_content = cleaned_content[7:]
            if cleaned_content.endswith('```'):
                cleaned_content = cleaned_content[:-3]
        elif cleaned_content.startswith('```'):
            cleaned_content = cleaned_content[3:]
            if cleaned_content.endswith('```'):
                cleaned_content = cleaned_content[:-3]
        
        return cleaned_content.strip()
    
    def calculate_grade(self, total_score: int, total_points: int = 100) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ê³„ì‚°"""
        if not self.exam_config:
            percentage = (total_score / total_points) * 100
        else:
            grade_scale = self.exam_config.get('grade_scale', {})
            if grade_scale:
                for grade, min_score in sorted(grade_scale.items(), key=lambda x: x[1], reverse=True):
                    if total_score >= min_score:
                        return grade
            percentage = (total_score / total_points) * 100
        
        # ê¸°ë³¸ ë“±ê¸‰ ì²´ê³„
        if percentage >= 95: return "A+"
        elif percentage >= 90: return "A"
        elif percentage >= 85: return "B+"
        elif percentage >= 80: return "B"
        elif percentage >= 75: return "C+"
        elif percentage >= 70: return "C"
        elif percentage >= 65: return "D+"
        elif percentage >= 60: return "D"
        else: return "F"
    
    async def grade_single_answer(self, filename: str) -> Dict[str, Any]:
        """ê°œë³„ ë‹µì•ˆ ì±„ì """
        print(f"\nğŸ“ ì±„ì  ì¤‘: {filename}")
        
        # íŒŒì¼ ì½ê¸°
        file_data = await self.read_answer_file(filename)
        if not file_data or 'content' not in file_data:
            return {
                'filename': filename,
                'error': 'íŒŒì¼ ì½ê¸° ì‹¤íŒ¨',
                'total_score': 0
            }
        
        print(f"   ğŸ“„ íŒŒì¼ í¬ê¸°: {file_data.get('size', 0)} bytes")
        
        # ì±„ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.build_grading_prompt(file_data['content'])
        
        try:
            print(f"   ğŸ¤– OpenAI API í˜¸ì¶œ ì¤‘...")
            
            # OpenAI API í˜¸ì¶œ
            response = await self.openai_client.chat.completions.create(
                model=os.getenv('OPENAI_MODEL', 'gpt-4'),
                messages=[{"role": "user", "content": prompt}],
                temperature=float(os.getenv('OPENAI_TEMPERATURE', '0.1'))
            )
            
            response_content = response.choices[0].message.content
            print(f"   âœ… API ì‘ë‹µ ë°›ìŒ")
            
            # JSON ì¶”ì¶œ ë° íŒŒì‹±
            cleaned_content = self.clean_json_response(response_content)
            
            try:
                result = json.loads(cleaned_content)
                
                # ë“±ê¸‰ì´ ì—†ìœ¼ë©´ ê³„ì‚°í•´ì„œ ì¶”ê°€
                if 'grade' not in result or not result['grade']:
                    total_points = self.exam_config.get('total_points', 100) if self.exam_config else 100
                    result['grade'] = self.calculate_grade(result.get('total_score', 0), total_points)
                
                print(f"   âœ… ì±„ì  ì™„ë£Œ: {result.get('total_score', 0)}ì  ({result.get('grade', 'N/A')})")
                
            except json.JSONDecodeError as json_err:
                print(f"   âš ï¸  JSON íŒŒì‹± ì‹œë„ ì¤‘...")
                # ì •ê·œì‹ìœ¼ë¡œ JSON ì¶”ì¶œ ì‹œë„
                json_match = re.search(r'\{.*\}', cleaned_content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        
                        # ë“±ê¸‰ ê³„ì‚°
                        if 'grade' not in result or not result['grade']:
                            total_points = self.exam_config.get('total_points', 100) if self.exam_config else 100
                            result['grade'] = self.calculate_grade(result.get('total_score', 0), total_points)
                        
                        print(f"   âœ… ì±„ì  ì™„ë£Œ (ì •ê·œì‹): {result.get('total_score', 0)}ì  ({result.get('grade', 'N/A')})")
                    except json.JSONDecodeError:
                        print(f"   âŒ JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨")
                        result = {
                            "total_score": 0,
                            "overall_feedback": f"JSON íŒŒì‹± ì‹¤íŒ¨. ì›ì‹œ ì‘ë‹µ: {cleaned_content[:200]}...",
                            "error": "JSON format error",
                            "grade": "F"
                        }
                else:
                    print(f"   âŒ JSON êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    result = {
                        "total_score": 0,
                        "overall_feedback": f"JSON êµ¬ì¡° ì—†ìŒ. ì‘ë‹µ: {cleaned_content[:200]}...",
                        "error": "No JSON structure found",
                        "grade": "F"
                    }
            
            result['filename'] = filename
            return result
            
        except Exception as e:
            print(f"   âŒ ì±„ì  ì˜¤ë¥˜: {e}")
            return {
                'filename': filename,
                'error': str(e),
                'total_score': 0,
                'grade': 'F'
            }
    
    async def batch_grade_answers(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë‹µì•ˆ ì¼ê´„ ì±„ì """
        print("ğŸš€ ì¼ê´„ ì±„ì  ì‹œì‘\n")
        
        # ì„¤ì • ë¡œë“œ
        if not await self.load_exam_config():
            return []
        
        # ë‹µì•ˆ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        answer_files = await self.get_answer_files()
        if not answer_files:
            return []
        
        print(f"\nğŸ“Š {len(answer_files)}ê°œ ë‹µì•ˆ ì±„ì  ì‹œì‘...")
        print("=" * 50)
        
        # ìˆœì°¨ì ìœ¼ë¡œ ì±„ì 
        results = []
        for i, file_info in enumerate(answer_files):
            print(f"\nğŸ“ˆ ì§„í–‰ë¥ : {i+1}/{len(answer_files)}")
            result = await self.grade_single_answer(file_info['filename'])
            results.append(result)
            
            # API ì œí•œ ë°©ì§€
            if i < len(answer_files) - 1:
                print(f"   â±ï¸  ì ì‹œ ëŒ€ê¸° ì¤‘... (API ì œí•œ ë°©ì§€)")
                await asyncio.sleep(2)
        
        self.results = results
        print("\nğŸ‰ ëª¨ë“  ë‹µì•ˆ ì±„ì  ì™„ë£Œ!")
        return results
    
    def generate_summary_report(self) -> Dict[str, Any]:
        """ì±„ì  ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        if not self.results:
            return {"error": "ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        valid_results = [r for r in self.results if 'total_score' in r and 'error' not in r]
        
        if not valid_results:
            return {"error": "ìœ íš¨í•œ ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        scores = [r['total_score'] for r in valid_results]
        total_points = self.exam_config.get('total_points', 100) if self.exam_config else 100
        
        # ë“±ê¸‰ë³„ ë¶„í¬
        grade_distribution = {}
        for result in valid_results:
            grade = result.get('grade', 'F')
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
        
        # ë¬¸ì œë³„ í‰ê·  ì ìˆ˜
        question_averages = {}
        if valid_results and 'question_scores' in valid_results[0]:
            questions = valid_results[0]['question_scores']
            for q in questions:
                q_num = q.get('question_number', 0)
                scores_for_q = []
                for result in valid_results:
                    q_scores = result.get('question_scores', [])
                    for qs in q_scores:
                        if qs.get('question_number') == q_num:
                            scores_for_q.append(qs.get('score', 0))
                            break
                if scores_for_q:
                    question_averages[f"ë¬¸ì œ_{q_num}"] = round(sum(scores_for_q) / len(scores_for_q), 1)
        
        return {
            "ì‹œí—˜_ì •ë³´": {
                "ì‹œí—˜ëª…": self.exam_config.get('exam_title', 'ì‹œí—˜') if self.exam_config else 'ì‹œí—˜',
                "ì´ì ": total_points,
                "ì‘ì‹œì_ìˆ˜": len(valid_results),
                "ì±„ì _ì™„ë£Œ_ì‹œê°„": asyncio.get_event_loop().time()
            },
            "ì ìˆ˜_í†µê³„": {
                "í‰ê· _ì ìˆ˜": round(sum(scores) / len(scores), 2),
                "ìµœê³ _ì ìˆ˜": max(scores),
                "ìµœì €_ì ìˆ˜": min(scores),
                "í‘œì¤€í¸ì°¨": round((sum((x - sum(scores)/len(scores))**2 for x in scores) / len(scores))**0.5, 2),
                "ì¤‘ê°„ê°’": sorted(scores)[len(scores)//2]
            },
            "ë“±ê¸‰_ë¶„í¬": grade_distribution,
            "ì ìˆ˜_êµ¬ê°„ë³„_ë¶„í¬": {
                f"{int(total_points*0.9)}ì _ì´ìƒ": len([s for s in scores if s >= total_points*0.9]),
                f"{int(total_points*0.8)}-{int(total_points*0.9-1)}ì ": len([s for s in scores if total_points*0.8 <= s < total_points*0.9]),
                f"{int(total_points*0.7)}-{int(total_points*0.8-1)}ì ": len([s for s in scores if total_points*0.7 <= s < total_points*0.8]),
                f"{int(total_points*0.6)}-{int(total_points*0.7-1)}ì ": len([s for s in scores if total_points*0.6 <= s < total_points*0.7]),
                f"{int(total_points*0.6)}ì _ë¯¸ë§Œ": len([s for s in scores if s < total_points*0.6])
            },
            "ë¬¸ì œë³„_í‰ê· ì ìˆ˜": question_averages
        }
    
    async def save_results(self, filename: str = "grading_results.json") -> bool:
        """ì±„ì  ê²°ê³¼ ì €ì¥"""
        print(f"ğŸ’¾ ì±„ì  ê²°ê³¼ ì €ì¥ ì¤‘... ({filename})")
        try:
            report_data = {
                "exam_config": self.exam_config,
                "summary": self.generate_summary_report(),
                "detailed_results": self.results,
                "grading_metadata": {
                    "grading_system": "MCP-based Exam Grading System",
                    "total_files_processed": len(self.results),
                    "successful_gradings": len([r for r in self.results if 'error' not in r]),
                    "failed_gradings": len([r for r in self.results if 'error' in r])
                }
            }
            
            result = await self.call_mcp_server("save_grading_result", {
                "results": report_data,
                "filename": filename
            })
            
            if "error" in result:
                print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {result['error']}")
                return False
            
            if result.get("success"):
                print(f"âœ… ì±„ì  ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result.get('saved_to', filename)}")
                return True
            else:
                print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {result}")
                return False
                    
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    async def get_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
        result = await self.call_mcp_server("get_system_info")
        if "error" not in result and result.get("success"):
            return result["system"]
        return {}
    
    def print_detailed_results(self, results: List[Dict[str, Any]], max_results: int = 5):
        """ìƒì„¸ ê²°ê³¼ ì¶œë ¥"""
        print("\nğŸ“‹ === ê°œë³„ ì±„ì  ê²°ê³¼ ===")
        
        valid_results = [r for r in results if 'error' not in r]
        error_results = [r for r in results if 'error' in r]
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        valid_results.sort(key=lambda x: x.get('total_score', 0), reverse=True)
        
        print(f"\nğŸ† ìƒìœ„ {min(max_results, len(valid_results))}ê°œ ê²°ê³¼:")
        for i, result in enumerate(valid_results[:max_results]):
            print(f"\n{i+1}. ğŸ“„ íŒŒì¼: {result['filename']}")
            print(f"   ğŸ“Š ì´ì : {result['total_score']}ì ")
            print(f"   ğŸ¯ ë“±ê¸‰: {result.get('grade', 'N/A')}")
            print(f"   ğŸ’¬ ì „ì²´ í”¼ë“œë°±: {result.get('overall_feedback', 'N/A')[:100]}...")
            
            # ë¬¸ì œë³„ ì ìˆ˜
            if 'question_scores' in result and result['question_scores']:
                print(f"   ğŸ“ ë¬¸ì œë³„ ì ìˆ˜:")
                for q in result['question_scores']:
                    q_num = q.get('question_number', '?')
                    score = q.get('score', 0)
                    max_score = q.get('max_score', 0)
                    print(f"      ë¬¸ì œ {q_num}: {score}/{max_score}ì ")
            
            # ê¸°ì¤€ë³„ ì ìˆ˜
            if 'criteria_scores' in result and result['criteria_scores']:
                print(f"   ğŸ“ ê¸°ì¤€ë³„ ì ìˆ˜:")
                for criteria, score in result['criteria_scores'].items():
                    print(f"      {criteria}: {score}ì ")
        
        # ì˜¤ë¥˜ ê²°ê³¼
        if error_results:
            print(f"\nâš ï¸  ì˜¤ë¥˜ ë°œìƒ íŒŒì¼: {len(error_results)}ê°œ")
            for err in error_results:
                print(f"   âŒ {err.get('filename', 'Unknown')}: {err.get('error', 'Unknown error')}")
    
    async def run_grading(self):
        """ì „ì²´ ì±„ì  í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ¯ MCP ê¸°ë°˜ ì‹œí—˜ ì±„ì  ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        system_info = await self.get_system_info()
        if system_info:
            print(f"ğŸ–¥ï¸  í”Œë«í¼: {system_info.get('platform', 'Unknown')}")
            print(f"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {system_info.get('working_directory', 'Unknown')}")
            print(f"ğŸ“‚ ë‹µì•ˆ ë””ë ‰í† ë¦¬: {system_info.get('exam_directory', 'Unknown')}")
        
        # ì±„ì  ì‹¤í–‰
        results = await self.batch_grade_answers()
        
        if results:
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            summary = self.generate_summary_report()
            print("\nğŸ“Š === ì±„ì  ê²°ê³¼ ìš”ì•½ ===")
            print("=" * 50)
            print(json.dumps(summary, ensure_ascii=False, indent=2))
            
            # ê²°ê³¼ ì €ì¥
            await self.save_results()
            
            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
            self.print_detailed_results(results)
            
            print("\nğŸ‰ ì±„ì  ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ!")
            print("=" * 60)
            
        else:
            print("âŒ ì±„ì í•  ë‹µì•ˆì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        client = ExamGradingClient()
        await client.run_grading()
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())
