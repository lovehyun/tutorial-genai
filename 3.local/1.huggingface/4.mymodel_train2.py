import os
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict

# 1. í•™ìŠµ ë°ì´í„° í™•ì¥ (ê¸ì • 4ê°œ, ë¶€ì • 4ê°œ)
train_data = {
    "text": [
        "I love this!", "This is terrible!", "I am happy.", "I am sad.",
        "This product is amazing!", "Worst experience ever.", "Absolutely fantastic!", "I hate it."
    ],
    "label": [1, 0, 1, 0, 1, 0, 1, 0]  # ê¸ì •(1), ë¶€ì •(0)
}

# 2. í‰ê°€ ë°ì´í„° í™•ì¥ (ê¸ì • 2ê°œ, ë¶€ì • 2ê°œ)
eval_data = {
    "text": [
        "I feel great today!", "The service was awful.", 
        "I'm super excited about this!", "Not what I expected."
    ],
    "label": [1, 0, 1, 0]  # ê¸ì •(1), ë¶€ì •(0)
}

# 3. Hugging Face `datasets` ë³€í™˜
train_dataset = Dataset.from_dict(train_data)
eval_dataset = Dataset.from_dict(eval_data)

# 4. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 5. ë°ì´í„°ì…‹ì„ í† í°í™”
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# 6. Hugging Face `DatasetDict`ë¡œ ë³€í™˜
split_dataset = DatasetDict({
    "train": tokenized_train_dataset,
    "eval": tokenized_eval_dataset
})

# 7. ê°ì„± ë¶„ì„ ëª¨ë¸ ìƒì„±
# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# 7. ê°ì„± ë¶„ì„ ëª¨ë¸ ìƒì„± (id2label ì¶”ê°€)
# id2label (ìˆ«ì->ë¼ë²¨): ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìˆ«ì IDë¥¼ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ë¡œ ë³€í™˜
# label2id (ë¼ë²¨ â†’ ìˆ«ì): í›ˆë ¨ ë°ì´í„°ì—ì„œ ì‚¬ëŒì´ ì •ì˜í•œ ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    id2label={0: "NEGATIVE", 1: "POSITIVE"},  # ë¼ë²¨ ì´ë¦„ ì§€ì •
    label2id={"NEGATIVE": 0, "POSITIVE": 1}
)

# 8. í›ˆë ¨ ì„¤ì • (FutureWarning í•´ê²°: `eval_strategy` ì‚¬ìš©)
output_dir = "./my_model"  # ì¤‘ê°„ íŒŒì¼ê³¼ ìµœì¢… ëª¨ë¸ ì €ì¥ í´ë” í†µì¼
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,  # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ 3 ì—í¬í¬ ìˆ˜í–‰
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],
    eval_dataset=split_dataset["eval"]
)

# 9. ëª¨ë¸ í•™ìŠµ
trainer.train()

# 10. í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ ì €ì¥
save_path = "./my_local_model"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"âœ… ëª¨ë¸ì´ {save_path} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
