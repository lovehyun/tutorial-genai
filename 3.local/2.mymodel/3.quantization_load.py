import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# 1. ì›ë˜ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡° ìƒì„± (float32 ìƒíƒœ)
model_name = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 2. ëª¨ë¸ì„ ë‹¤ì‹œ ì–‘ìí™”í•˜ì—¬ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë³€í™˜
model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# 3. ì–‘ìí™”ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
load_path = "./quantized_model"
checkpoint = torch.load(f"{load_path}/pytorch_model.bin")

# 4. ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ (strict=False ì¶”ê°€)
model.load_state_dict(checkpoint["model_state_dict"], strict=False)

# 5. ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ ë³µì› (classifier.weight & classifier.bias)
model.classifier.weight = torch.nn.Parameter(checkpoint["classifier_weight"])
model.classifier.bias = torch.nn.Parameter(checkpoint["classifier_bias"])

# 6. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½ (í•„ìˆ˜)
model.eval()

# 7. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(load_path)

# 8. ê°ì„± ë¶„ì„ ì‹¤í–‰ (PyTorch ì§ì ‘ ì‚¬ìš©)
def predict(text):
    inputs = tokenizer(text, return_tensors="pt")  # ì…ë ¥ì„ í…ì„œë¡œ ë³€í™˜
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡ ìš©)
        outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
    labels = {0: "NEGATIVE", 1: "POSITIVE"}
    return labels[predicted_class]

# 9. ì˜ˆì œ ë¬¸ì¥ í…ŒìŠ¤íŠ¸
test_sentences = [
    "I love using my own AI model!",
    "This is the worst experience ever.",
    "I am extremely happy today!",
    "I feel so bad...",
    "This app is fantastic!",
    "Not impressed at all.",
    "Absolutely love this movie!",
    "What a waste of money."
]

# 10. ê²°ê³¼ ì¶œë ¥
for text in test_sentences:
    result = predict(text)
    print(f"ğŸ“¢ Input: {text} â†’ ğŸ¯ Prediction: {result}")
