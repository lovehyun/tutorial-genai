import os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (OpenAI API í‚¤)
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ê²½ê³  ì œê±° (OpenMP ì¶©ëŒ í•´ê²°)
# os.environ["USE_SIMPLE_THREADED_LEVEL3"] = "1"
# os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ë” ì¢‹ì€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
embedding_model = SentenceTransformer("all-mpnet-base-v2")

# ë¬¸ì„œ ë°ì´í„° (FAISSì— ì €ì¥í•  ë°ì´í„°)
documents = [
    "Pythonì€ ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "OpenAIëŠ” AI ì—°êµ¬ë¥¼ ì„ ë„í•˜ëŠ” ê¸°ì—…ì…ë‹ˆë‹¤.",
    "FAISSëŠ” ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
]

# ë²¡í„° ì •ê·œí™” í•¨ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìš©)
def normalize_vector(vec):
    return vec / np.linalg.norm(vec, axis=1, keepdims=True)  # ë²¡í„° ì •ê·œí™”

# ë¬¸ì„œ ì„ë² ë”© ìƒì„± (ì •ê·œí™” ì¶”ê°€)
doc_embeddings = embedding_model.encode(documents)
doc_embeddings = normalize_vector(doc_embeddings)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìš©

# FAISS ì¸ë±ìŠ¤ ìƒì„± (ë‚´ì  ê¸°ë°˜, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìš©)
embedding_dim = doc_embeddings.shape[1]
index = faiss.IndexFlatIP(embedding_dim)  # **L2 ëŒ€ì‹  IndexFlatIP ì‚¬ìš©**
index.add(doc_embeddings)

# ê²€ìƒ‰ + OpenAI API í™œìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def rag_query(user_query):
    # ë¡œì»¬ì—ì„œ ì„ë² ë”© ë³€í™˜ (ì •ê·œí™” ì ìš©)
    query_embedding = embedding_model.encode([user_query])
    query_embedding = normalize_vector(query_embedding)  # ì •ê·œí™” í•„ìˆ˜
    
    # FAISS ê²€ìƒ‰ (ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ ì°¾ê¸°)
    distances, indices = index.search(query_embedding, k=1)
    retrieved_doc = documents[indices[0][0]]
    
    # Inner Product ê¸°ë°˜ FAISS ì ìˆ˜ë¥¼ ë³´ì •í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜
    similarity_score = (distances[0][0] + 1) / 2  # ë³´ì • í›„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”

    print("\nğŸ” FAISS ê²€ìƒ‰ ê²°ê³¼:")
    print(f"   ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ: {retrieved_doc}")
    print(f"   ğŸ¯ ê²€ìƒ‰ ì •í™•ë„(ìœ ì‚¬ë„ ì ìˆ˜): {similarity_score:.4f}")  # 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³´ì •

    # OpenAI APIë¥¼ í™œìš©í•œ ìµœì¢… ì‘ë‹µ ìƒì„±
    prompt = f"""
    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ì¶”ê°€ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì œê³µëœ ì •ë³´ ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.

    ì°¸ê³  ì •ë³´:
    {retrieved_doc}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_query}
    ë‹µë³€:
    """

    print("\nğŸ“ OpenAI APIì—ê²Œ ì „ë‹¬ëœ í”„ë¡¬í”„íŠ¸:")
    print(prompt)

    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ]
    )

    return response.choices[0].message.content

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
query = "OpenAIëŠ” ì–´ë–¤ ê¸°ì—…ì¸ê°€ìš”?"
print("\nğŸ’¬ GPT ì‘ë‹µ:\n", rag_query(query))
