import os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (OpenAI API í‚¤)
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ê²½ê³  ì œê±° (OpenMP ì¶©ëŒ í•´ê²°)
os.environ["USE_SIMPLE_THREADED_LEVEL3"] = "1"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ (Hugging Face ì‚¬ìš©)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # ê°€ë²¼ìš´ ëª¨ë¸

# ë¬¸ì„œ ë°ì´í„° (FAISSì— ì €ì¥í•  ë°ì´í„°)
documents = [
    "Pythonì€ ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "OpenAIëŠ” AI ì—°êµ¬ë¥¼ ì„ ë„í•˜ëŠ” ê¸°ì—…ì…ë‹ˆë‹¤.",
    "FAISSëŠ” ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
]

# ë²¡í„° ì •ê·œí™” í•¨ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìš©)
def normalize_vector(vec):
    return vec / np.linalg.norm(vec, axis=1, keepdims=True)  # ë²¡í„° ì •ê·œí™”

# ë¬¸ì„œ ì„ë² ë”© ìƒì„± (ì •ê·œí™” ì¶”ê°€)
doc_embeddings = embedding_model.encode(documents)
doc_embeddings = normalize_vector(doc_embeddings)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´ ì •ê·œí™” í•„ìˆ˜

# FAISS ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
# FAISS ì¸ë±ìŠ¤	ì˜ë¯¸	ê±°ë¦¬ ê³„ì‚° ë°©ì‹	ì£¼ìš” ìš©ë„
# IndexFlatL2	L2 ê±°ë¦¬ ê¸°ë°˜ ê²€ìƒ‰	ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean Distance)	ì´ë¯¸ì§€ ê²€ìƒ‰, ê³µê°„ ê²€ìƒ‰, ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
# IndexFlatIP	ë‚´ì (Inner Product) ê¸°ë°˜ ê²€ìƒ‰	ë‚´ì  (dot product)	ë¬¸ì„œ ê²€ìƒ‰, ì¶”ì²œ ì‹œìŠ¤í…œ, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰

embedding_dim = doc_embeddings.shape[1]
index = faiss.IndexFlatIP(embedding_dim)  # L2 ëŒ€ì‹  IndexFlatIP ì‚¬ìš©
index.add(doc_embeddings)

# ê²€ìƒ‰ + OpenAI API í™œìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def rag_query(user_query):
    # ë¡œì»¬ì—ì„œ ì„ë² ë”© ë³€í™˜ (ì •ê·œí™” ì ìš©)
    query_embedding = normalize_vector(embedding_model.encode([user_query]))
    
    # FAISS ê²€ìƒ‰ (ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ ì°¾ê¸°)
    distances, indices = index.search(query_embedding, k=1)
    retrieved_doc = documents[indices[0][0]]
    
    # Inner Productë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜
    # FAISSì˜ ë‚´ì  ì ìˆ˜ëŠ” (-1, 1) ë²”ìœ„ì´ë¯€ë¡œ (score + 1) / 2ë¡œ ë³´ì •í•´ì•¼ ë” ì •í™•í•œ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
    similarity_score = (distances[0][0] + 1) / 2  # ë³´ì • í›„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”

    print("\nğŸ” FAISS ê²€ìƒ‰ ê²°ê³¼:")
    print(f"   ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ: {retrieved_doc}")
    print(f"   ğŸ¯ ê²€ìƒ‰ ì •í™•ë„(ìœ ì‚¬ë„ ì ìˆ˜): {similarity_score:.4f}")

    # OpenAI APIë¥¼ í™œìš©í•œ ìµœì¢… ì‘ë‹µ ìƒì„±
    prompt = f"""
    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ì¶”ê°€ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì œê³µëœ ì •ë³´ ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.

    ì°¸ê³  ì •ë³´:
    {retrieved_doc}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_query}
    ë‹µë³€:
    """

    print("\nğŸ“ OpenAI APIì—ê²Œ ì „ë‹¬ëœ í”„ë¡¬í”„íŠ¸:")
    print(prompt)

    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ]
    )

    return response.choices[0].message.content

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
query = "OpenAIëŠ” ì–´ë–¤ ê¸°ì—…ì¸ê°€ìš”?"
print("\nğŸ’¬ GPT ì‘ë‹µ:\n", rag_query(query))
