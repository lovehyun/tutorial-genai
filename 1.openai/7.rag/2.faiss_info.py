# pip install openai faiss-cpu tiktoken python-dotenv

import os
import numpy as np
import faiss
import tiktoken  # OpenAIì˜ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ì˜ˆì œ ë¬¸ì„œ ë°ì´í„° (RAGë¥¼ ìœ„í•œ ê²€ìƒ‰ ëŒ€ìƒ)
documents = [
    "Pythonì€ ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "OpenAIëŠ” AI ì—°êµ¬ë¥¼ ì„ ë„í•˜ëŠ” ê¸°ì—…ì…ë‹ˆë‹¤.",
    "FAISSëŠ” ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
]

def count_tokens(text, model="gpt-4-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# ë¬¸ì„œ ì¸ë±ì‹± (ë²¡í„° ìƒì„± ë° ì €ì¥)
def get_embedding(text):
    response = client.embeddings.create(input=text,
    model="text-embedding-ada-002")
    return np.array(response.data[0].embedding)

# ë¬¸ì„œ ì„ë² ë”© ìƒì„± ë° ë²¡í„°DB(Faiss) êµ¬ì¶•
index = faiss.IndexFlatL2(1536)  # OpenAI ì„ë² ë”© ì°¨ì›(1536)
doc_embeddings = np.array([get_embedding(doc) for doc in documents])
index.add(doc_embeddings)

# ê²€ìƒ‰ ë° RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
def rag_query(user_query):
    query_embedding = get_embedding(user_query)
    distances, indices = index.search(np.array([query_embedding]), k=1) # FAISS ê²€ìƒ‰ (k=1: ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ 1ê°œ ì°¾ê¸°)
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì •í™•ë„(ê±°ë¦¬)
    retrieved_doc = documents[indices[0][0]]
    true_distance = np.sqrt(distances[0][0])  # FAISSëŠ” ì œê³±ëœ ê±°ë¦¬ê°’ì„ ë°˜í™˜í•˜ë¯€ë¡œ, ì œê³±ê·¼ì„ ì·¨í•´ì•¼ í•¨
    similarity_score = 1 / (1 + true_distance)  # ê±°ë¦¬ê°’ì„ ì •ê·œí™”í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ)

    print("\nğŸ” FAISS ê²€ìƒ‰ ê²°ê³¼:")
    print(f"   ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ: {retrieved_doc}")
    print(f"   ğŸ¯ ê²€ìƒ‰ ì •í™•ë„(ìœ ì‚¬ë„ ì ìˆ˜): {similarity_score:.4f}")

    # ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ë¼ë©´ ê²½ê³  ì¶œë ¥
    if similarity_score < 0.2:
        print("\nâš ï¸ ê²½ê³ : ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ í¬ê²Œ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

    # GPTì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""
    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ë‹¤ë¥¸ ì§€ì‹ì´ë‚˜ ì¶”ë¡ ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì£¼ì–´ì§„ ì •ë³´ ë‚´ì—ì„œë§Œ ë‹µì„ ìƒì„±í•˜ì„¸ìš”.

    ì°¸ê³  ì •ë³´:
    {retrieved_doc}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_query}
    ë‹µë³€:
    """

    print("\nğŸ“ GPTì—ê²Œ ì „ë‹¬ëœ í”„ë¡¬í”„íŠ¸:")
    print(prompt)

    # í† í° ìˆ˜ ê³„ì‚°
    token_count = count_tokens(prompt)
    print(f"\nğŸ“ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {token_count} í† í°")
    
    response = client.chat.completions.create(model="gpt-3.5-turbo",
    messages=[
        # {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."},
        # {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë°˜ëŒ€ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."},
        {"role": "user", "content": prompt}
    ])

    return response.choices[0].message.content

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
query = "OpenAIëŠ” ì–´ë–¤ ê¸°ì—…ì¸ê°€ìš”?"
print(rag_query(query))
