# pip install sentence-transformers faiss-cpu openai python-dotenv
# pip install --upgrade huggingface_hub

import os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (OpenAI API í‚¤)
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ê²½ê³  ì œê±°ìš©
# os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ (Hugging Face ì‚¬ìš©)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # ë¹ ë¥´ê³  ê°€ë²¼ìš´ ëª¨ë¸

# ë¬¸ì„œ ë°ì´í„° (FAISSì— ì €ì¥í•  ë°ì´í„°)
documents = [
    "Pythonì€ ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "OpenAIëŠ” AI ì—°êµ¬ë¥¼ ì„ ë„í•˜ëŠ” ê¸°ì—…ì…ë‹ˆë‹¤.",
    "FAISSëŠ” ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
]

# ë¡œì»¬ ì„ë² ë”© ìƒì„±
doc_embeddings = np.array(embedding_model.encode(documents))

# FAISS ì¸ë±ìŠ¤ ìƒì„±
embedding_dim = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(doc_embeddings)

# ê²€ìƒ‰ + OpenAI API í™œìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def rag_query(user_query):
    # ë¡œì»¬ì—ì„œ ì„ë² ë”© ë³€í™˜ (OpenAI API X)
    query_embedding = np.array([embedding_model.encode(user_query)])
    
    # FAISS ê²€ìƒ‰ (ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ ì°¾ê¸°)
    distances, indices = index.search(query_embedding, k=1)
    retrieved_doc = documents[indices[0][0]]
    similarity_score = 1 / (1 + distances[0][0])  # ìœ ì‚¬ë„ ì ìˆ˜ ë³€í™˜

    print("\nğŸ” FAISS ê²€ìƒ‰ ê²°ê³¼:")
    print(f"   ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ: {retrieved_doc}")
    print(f"   ğŸ¯ ê²€ìƒ‰ ì •í™•ë„(ìœ ì‚¬ë„ ì ìˆ˜): {similarity_score:.4f}")

    # OpenAI APIë¥¼ í™œìš©í•œ ìµœì¢… ì‘ë‹µ ìƒì„±
    prompt = f"""
    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ì¶”ê°€ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì œê³µëœ ì •ë³´ ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.

    ì°¸ê³  ì •ë³´:
    {retrieved_doc}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_query}
    ë‹µë³€:
    """

    print("\nğŸ“ OpenAI APIì—ê²Œ ì „ë‹¬ëœ í”„ë¡¬í”„íŠ¸:")
    print(prompt)

    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ]
    )

    return response.choices[0].message.content

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
query = "OpenAIëŠ” ì–´ë–¤ ê¸°ì—…ì¸ê°€ìš”?"
print("\nğŸ’¬ GPT ì‘ë‹µ:\n", rag_query(query))
